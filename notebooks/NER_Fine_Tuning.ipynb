{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 3: Fine-Tuning NER Model for Amharic E-commerce Data\n",
        "\n",
        "This notebook implements Task 3 from the project requirements:\n",
        "- Fine-tune a Named Entity Recognition (NER) model to extract key entities (products, prices, locations) from Amharic Telegram messages\n",
        "- Use pre-trained models like XLM-Roberta, bert-tiny-amharic, or afroxmlr\n",
        "- Load labeled dataset in CoNLL format\n",
        "- Tokenize and align labels\n",
        "- Train using Hugging Face's Trainer API\n",
        "- Evaluate and save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch transformers datasets tokenizers scikit-learn accelerate seqeval --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transformers and datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report as seq_classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load and Prepare CoNLL Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_conll_data(file_path):\n",
        "    \"\"\"Load CoNLL format data and convert to list of sentences with labels\"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            \n",
        "            # Skip comments\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "                \n",
        "            # Empty line indicates end of sentence\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    labels.append(current_labels)\n",
        "                    current_sentence = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                # Split token and label\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    token, label = parts\n",
        "                    current_sentence.append(token)\n",
        "                    current_labels.append(label)\n",
        "    \n",
        "    # Add last sentence if file doesn't end with empty line\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence)\n",
        "        labels.append(current_labels)\n",
        "    \n",
        "    return sentences, labels\n",
        "\n",
        "# Load the data\n",
        "conll_file = '../data/conll_labeled/amharic_ecommerce_conll.txt'\n",
        "sentences, labels = load_conll_data(conll_file)\n",
        "\n",
        "print(f\"Loaded {len(sentences)} sentences\")\n",
        "print(f\"Example sentence: {sentences[0]}\")\n",
        "print(f\"Example labels: {labels[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label mappings\n",
        "unique_labels = set()\n",
        "for label_list in labels:\n",
        "    unique_labels.update(label_list)\n",
        "\n",
        "label_list = sorted(list(unique_labels))\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"Unique labels: {label_list}\")\n",
        "print(f\"Number of labels: {len(label_list)}\")\n",
        "\n",
        "# Convert labels to numeric IDs\n",
        "label_ids = [[label2id[label] for label in label_list] for label_list in labels]\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    sentences, label_ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training sentences: {len(train_sentences)}\")\n",
        "print(f\"Validation sentences: {len(val_sentences)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Prepare Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'tokens': train_sentences,\n",
        "    'ner_tags': train_labels\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'tokens': val_sentences,\n",
        "    'ner_tags': val_labels\n",
        "})\n",
        "\n",
        "print(\"Datasets created successfully\")\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Validation dataset: {val_dataset}\")\n",
        "\n",
        "# Display some examples\n",
        "print(f\"\\nExample training data:\")\n",
        "for i in range(2):\n",
        "    print(f\"Sentence {i+1}: {train_sentences[i]}\")\n",
        "    print(f\"Labels {i+1}: {[id2label[label] for label in train_labels[i]]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Setup and Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name - using XLM-Roberta for multilingual support\n",
        "model_name = \"xlm-roberta-base\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Loaded tokenizer for {model_name}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Load the model for token classification\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\"Model loaded with {len(label_list)} labels\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    \"\"\"Tokenize the texts and align the labels with the tokens\"\"\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=False,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        \n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special tokens (CLS, SEP, PAD) get label -100\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # First token of a word gets the label\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # Subsequent tokens of the same word get -100 (ignored in loss calculation)\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"Tokenization completed\")\n",
        "print(f\"Train tokenized: {train_tokenized}\")\n",
        "print(f\"Validation tokenized: {val_tokenized}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Training Setup and Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = {\n",
        "        'precision': precision_score(true_labels, true_predictions),\n",
        "        'recall': recall_score(true_labels, true_predictions),\n",
        "        'f1': f1_score(true_labels, true_predictions),\n",
        "    }\n",
        "    return results\n",
        "\n",
        "print(\"Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/xlm-roberta-amharic-ner\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"../logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",  # Disable wandb logging\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(\"Trainer initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save the best model\n",
        "model_save_path = \"../models/xlm-roberta-amharic-ner-final\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save label mappings\n",
        "label_mappings = {\n",
        "    'id2label': id2label,\n",
        "    'label2id': label2id\n",
        "}\n",
        "\n",
        "with open(f\"{model_save_path}/label_mappings.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(label_mappings, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "print(f\"Label mappings saved to {model_save_path}/label_mappings.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
