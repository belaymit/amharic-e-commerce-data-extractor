{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 4: Model Comparison & Selection for Amharic NER\n",
        "\n",
        "This notebook implements Task 4 from the project requirements:\n",
        "- Compare multiple NER models (XLM-Roberta, DistilBERT, mBERT, etc.)\n",
        "- Evaluate on validation set for accuracy, speed, and robustness\n",
        "- Select the best-performing model based on evaluation metrics\n",
        "- Provide detailed analysis and recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch transformers datasets tokenizers scikit-learn accelerate seqeval --quiet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transformers and datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the same data processing functions from Task 3\n",
        "def load_conll_data(file_path):\n",
        "    \"\"\"Load CoNLL format data and convert to list of sentences with labels\"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    labels.append(current_labels)\n",
        "                    current_sentence = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    token, label = parts\n",
        "                    current_sentence.append(token)\n",
        "                    current_labels.append(label)\n",
        "    \n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence)\n",
        "        labels.append(current_labels)\n",
        "    \n",
        "    return sentences, labels\n",
        "\n",
        "# Load data\n",
        "conll_file = '../data/conll_labeled/amharic_ecommerce_conll.txt'\n",
        "sentences, labels = load_conll_data(conll_file)\n",
        "\n",
        "# Create label mappings\n",
        "unique_labels = set()\n",
        "for label_list in labels:\n",
        "    unique_labels.update(label_list)\n",
        "\n",
        "label_list = sorted(list(unique_labels))\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "# Convert to numeric IDs and split data\n",
        "label_ids = [[label2id[label] for label in label_list] for label_list in labels]\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    sentences, label_ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Data loaded: {len(sentences)} sentences, {len(label_list)} labels\")\n",
        "print(f\"Train: {len(train_sentences)}, Validation: {len(val_sentences)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to compare\n",
        "models_to_compare = {\n",
        "    'XLM-Roberta': 'xlm-roberta-base',\n",
        "    'DistilBERT': 'distilbert-base-multilingual-cased',\n",
        "    'mBERT': 'bert-base-multilingual-cased',\n",
        "    'XLM-Roberta-Large': 'xlm-roberta-large'  # Optional: if resources allow\n",
        "}\n",
        "\n",
        "# We'll start with the smaller models for faster comparison\n",
        "quick_models = {\n",
        "    'XLM-Roberta': 'xlm-roberta-base',\n",
        "    'DistilBERT': 'distilbert-base-multilingual-cased',\n",
        "    'mBERT': 'bert-base-multilingual-cased'\n",
        "}\n",
        "\n",
        "print(\"Models selected for comparison:\")\n",
        "for name, model_id in quick_models.items():\n",
        "    print(f\"- {name}: {model_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model training and comparison function\n",
        "def train_and_evaluate_model(model_name, model_id, train_dataset, val_dataset, id2label, label2id):\n",
        "    \"\"\"Train and evaluate a single model\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {model_name} ({model_id})\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_id,\n",
        "        num_labels=len(id2label),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    \n",
        "    # Tokenization function\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding=False,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            \n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "    val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # Evaluation metrics\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        true_predictions = [\n",
        "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        results = {\n",
        "            'precision': precision_score(true_labels, true_predictions),\n",
        "            'recall': recall_score(true_labels, true_predictions),\n",
        "            'f1': f1_score(true_labels, true_predictions),\n",
        "        }\n",
        "        return results\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"../models/{model_name.lower()}-amharic-ner\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,  # Reduced for comparison speed\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_dir=f\"../logs/{model_name.lower()}\",\n",
        "        logging_steps=10,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    trainer.train()\n",
        "    \n",
        "    # Evaluate\n",
        "    eval_results = trainer.evaluate()\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    print(f\"Results: F1={eval_results['eval_f1']:.4f}, P={eval_results['eval_precision']:.4f}, R={eval_results['eval_recall']:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'model_id': model_id,\n",
        "        'training_time': training_time,\n",
        "        'f1_score': eval_results['eval_f1'],\n",
        "        'precision': eval_results['eval_precision'],\n",
        "        'recall': eval_results['eval_recall'],\n",
        "        'model': model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({'tokens': train_sentences, 'ner_tags': train_labels})\n",
        "val_dataset = Dataset.from_dict({'tokens': val_sentences, 'ner_tags': val_labels})\n",
        "\n",
        "# Run comparison for all models\n",
        "results = []\n",
        "\n",
        "for model_name, model_id in quick_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training and evaluating: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        result = train_and_evaluate_model(\n",
        "            model_name, model_id, train_dataset, val_dataset, id2label, label2id\n",
        "        )\n",
        "        results.append(result)\n",
        "        \n",
        "        # Save intermediate results\n",
        "        pd.DataFrame(results).to_csv(f'../model_comparison_partial.csv', index=False)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error training {model_name}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nCompleted training {len(results)} models successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comparison results and analysis\n",
        "if results:\n",
        "    # Create comparison DataFrame\n",
        "    comparison_df = pd.DataFrame(results)\n",
        "    comparison_df = comparison_df.sort_values('f1_score', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL COMPARISON RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(comparison_df[['model_name', 'f1_score', 'precision', 'recall', 'training_time']].to_string(index=False))\n",
        "    \n",
        "    # Save results\n",
        "    comparison_df.to_csv('../model_comparison_results.csv', index=False)\n",
        "    \n",
        "    # Best model analysis\n",
        "    best_model = comparison_df.iloc[0]\n",
        "    print(f\"\\n BEST MODEL: {best_model['model_name']}\")\n",
        "    print(f\"F1 Score: {best_model['f1_score']:.4f}\")\n",
        "    print(f\"Precision: {best_model['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_model['recall']:.4f}\")\n",
        "    print(f\"Training Time: {best_model['training_time']:.2f} seconds\")\n",
        "    \n",
        "    # Performance analysis\n",
        "    print(f\"\\n PERFORMANCE ANALYSIS:\")\n",
        "    print(f\"• Best F1 Score: {comparison_df['f1_score'].max():.4f}\")\n",
        "    print(f\"• Average F1 Score: {comparison_df['f1_score'].mean():.4f}\")\n",
        "    print(f\"• Fastest Training: {comparison_df.loc[comparison_df['training_time'].idxmin(), 'model_name']} ({comparison_df['training_time'].min():.2f}s)\")\n",
        "    print(f\"• Average Training Time: {comparison_df['training_time'].mean():.2f} seconds\")\n",
        "    \n",
        "    print(f\"\\n Task 4 completed! Results saved to model_comparison_results.csv\")\n",
        "else:\n",
        "    print(\"No models were successfully trained. Please check the setup and try again.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
