{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 5: Model Interpretability for Amharic NER\n",
        "\n",
        "This notebook implements Task 5 from the project requirements:\n",
        "- Use SHAP (SHapley Additive exPlanations) to interpret model predictions\n",
        "- Use LIME (Local Interpretable Model-agnostic Explanations) for local explanations\n",
        "- Analyze difficult cases where the model struggles\n",
        "- Generate comprehensive reports on model decision-making\n",
        "- Identify areas for improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch transformers datasets shap lime matplotlib seaborn plotly --quiet\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForTokenClassification,\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best performing model (assuming from Task 4)\n",
        "def load_trained_model(model_path=\"../models/xlm-roberta-amharic-ner-final\"):\n",
        "    \"\"\"Load a trained model and tokenizer\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        \n",
        "        # Load label mappings\n",
        "        with open(f\"{model_path}/label_mappings.json\", 'r', encoding='utf-8') as f:\n",
        "            label_mappings = json.load(f)\n",
        "            \n",
        "        return model, tokenizer, label_mappings\n",
        "    except:\n",
        "        print(\"Trained model not found. Please run Task 3 first.\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load model\n",
        "model, tokenizer, label_mappings = load_trained_model()\n",
        "\n",
        "if model is not None:\n",
        "    print(\"Model loaded successfully!\")\n",
        "    print(f\"Labels: {list(label_mappings['label2id'].keys())}\")\n",
        "else:\n",
        "    print(\"Please train a model first using Task 3 notebook\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Entity Prediction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_entities(text, model, tokenizer):\n",
        "    \"\"\"Predict entities in a given text\"\"\"\n",
        "    if model is None:\n",
        "        print(\"No model loaded. Please run Task 3 first.\")\n",
        "        return []\n",
        "        \n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_token_class = torch.argmax(predictions, dim=-1)\n",
        "    \n",
        "    # Convert to labels\n",
        "    id2label = model.config.id2label\n",
        "    predicted_labels = [id2label[pred.item()] for pred in predicted_token_class[0]]\n",
        "    \n",
        "    # Get tokens\n",
        "    tokens_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
        "    \n",
        "    # Filter out special tokens\n",
        "    filtered_tokens_labels = []\n",
        "    for token, label in zip(tokens_list, predicted_labels):\n",
        "        if token not in ['<s>', '</s>', '<pad>', '<unk>']:\n",
        "            filtered_tokens_labels.append((token, label))\n",
        "    \n",
        "    return filtered_tokens_labels\n",
        "\n",
        "# Test sentences for analysis\n",
        "test_sentences = [\n",
        "    \"ቦርሳ በጣም ጥሩ! ዋጋ 5000 ብር። ቦሌ ውስጥ ይገኛል።\",\n",
        "    \"cream በጣም ጥሩ! ዋጋ 1200 ብር። መርካቶ ውስጥ ይገኛል።\",\n",
        "    \"ጫማ እና ሻርቶች ዋጋ 2500 ብር። አዲስ አበባ ውስጥ ይገኛል።\",\n",
        "    \"iPhone በጣም ቆንጆ! ዋጋ 45000 ብር። ፒያሳ ውስጥ ይገኛል።\"\n",
        "]\n",
        "\n",
        "if model is not None:\n",
        "    print(\"Testing entity prediction on sample texts:\")\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        print(f\"\\nSentence {i+1}: {sentence}\")\n",
        "        predictions = predict_entities(sentence, model, tokenizer)\n",
        "        print(\"Predicted entities:\")\n",
        "        for token, label in predictions:\n",
        "            if label != 'O':\n",
        "                print(f\"  {token:<15} -> {label}\")\n",
        "else:\n",
        "    print(\"Model not available. Please run Task 3 first to train a model.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
