{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 2: CoNLL Format Labeling for Amharic NER\n",
        "\n",
        "##  Objective\n",
        "Create **CoNLL format labeled data** from Amharic e-commerce messages for Named Entity Recognition (NER) training.\n",
        "\n",
        "##  Requirements\n",
        "- Label **30-50 messages** in CoNLL format\n",
        "- Entity types: **Product**, **Location**, **Price**  \n",
        "- Save as plain text file for NER training\n",
        "\n",
        "##  Entity Types\n",
        "- `B-PRODUCT`, `I-PRODUCT`: Product names\n",
        "- `B-LOC`, `I-LOC`: Location names\n",
        "- `B-PRICE`, `I-PRICE`: Price expressions\n",
        "- `O`: Outside any entity\n",
        "\n",
        "## Expected Output\n",
        "```\n",
        "# Message 1: ·ã®·à¥·â∂·âΩ ·â¶·à≠·à≥ ·ãã·åã 2500 ·â•·à≠ ·â†·ä†·ã≤·àµ ·ä†·â†·â£\n",
        "·ã®·à¥·â∂·âΩ\tO\n",
        "·â¶·à≠·à≥\tB-PRODUCT\n",
        "·ãã·åã\tB-PRICE\n",
        "2500\tI-PRICE\n",
        "·â•·à≠\tI-PRICE\n",
        "·â†·ä†·ã≤·àµ\tO\n",
        "·ä†·ã≤·àµ\tB-LOC\n",
        "·ä†·â†·â£\tI-LOC\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Task 2: CoNLL Format Labeling for Amharic NER\n",
            "============================================================\n",
            "Setting up CoNLL labeling system...\n",
            " Project root: /home/btd/Documents/KAIM/amharic-e-commerce-data-extractor\n",
            "Ready to start CoNLL labeling!\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup and Configuration\n",
        "import sys\n",
        "import sqlite3\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\" Task 2: CoNLL Format Labeling for Amharic NER\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Setting up CoNLL labeling system...\")\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\" Project root: {project_root}\")\n",
        "print(\"Ready to start CoNLL labeling!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading data for CoNLL labeling...\n",
            " Found database: /home/btd/Documents/KAIM/amharic-e-commerce-data-extractor/data/processed/amharic_ecommerce.db\n",
            "   Available tables: []\n",
            " Found database: /home/btd/Documents/KAIM/amharic-e-commerce-data-extractor/data/historical_messages.db\n",
            "   Available tables: ['historical_messages', 'collection_stats']\n",
            "   Loaded 50 messages from historical_messages\n",
            "\n",
            " Data Summary:\n",
            "   Messages loaded: 50\n",
            "   Sample fields: ['id', 'channel', 'channel_title', 'text', 'date', 'year', 'month', 'views', 'has_media', 'collection_timestamp']\n",
            "   Text field: 'text'\n",
            "   Sample text: cream ·â†·å£·àù ·å•·à©! ·ãã·åã 1200 ·â•·à≠·ç¢ ·àò·à≠·ä´·â∂ ·ãç·àµ·å• ·ã≠·åà·äõ·àç·ç¢...\n"
          ]
        }
      ],
      "source": [
        "# 2. Load and Preview Data\n",
        "print(\" Loading data for CoNLL labeling...\")\n",
        "\n",
        "# Try to load from existing databases\n",
        "data_sources = [\n",
        "    project_root / \"data/processed/amharic_ecommerce.db\",\n",
        "    project_root / \"data/historical_messages.db\", \n",
        "    project_root / \"data/demo.db\"\n",
        "]\n",
        "\n",
        "messages = []\n",
        "for db_path in data_sources:\n",
        "    if db_path.exists():\n",
        "        print(f\" Found database: {db_path}\")\n",
        "        try:\n",
        "            conn = sqlite3.connect(db_path)\n",
        "            cursor = conn.cursor()\n",
        "            \n",
        "            # Check available tables\n",
        "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "            tables = [row[0] for row in cursor.fetchall()]\n",
        "            print(f\"   Available tables: {tables}\")\n",
        "            \n",
        "            # Try to load messages from any available table\n",
        "            for table in ['processed_messages', 'historical_messages', 'messages']:\n",
        "                if table in tables:\n",
        "                    cursor.execute(f\"SELECT * FROM {table} LIMIT 50\")\n",
        "                    rows = cursor.fetchall()\n",
        "                    if rows:\n",
        "                        cursor.execute(f\"PRAGMA table_info({table})\")\n",
        "                        columns = [row[1] for row in cursor.fetchall()]\n",
        "                        messages = [dict(zip(columns, row)) for row in rows]\n",
        "                        print(f\"   Loaded {len(messages)} messages from {table}\")\n",
        "                        break\n",
        "            \n",
        "            conn.close()\n",
        "            if messages:\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   Error loading from {db_path}: {e}\")\n",
        "\n",
        "print(f\"\\n Data Summary:\")\n",
        "print(f\"   Messages loaded: {len(messages)}\")\n",
        "if messages:\n",
        "    sample_msg = messages[0]\n",
        "    print(f\"   Sample fields: {list(sample_msg.keys())}\")\n",
        "    text_field = None\n",
        "    for field in ['text', 'original_text', 'message']:\n",
        "        if field in sample_msg:\n",
        "            text_field = field\n",
        "            break\n",
        "    \n",
        "    if text_field:\n",
        "        print(f\"   Text field: '{text_field}'\")\n",
        "        print(f\"   Sample text: {sample_msg[text_field][:100]}...\")\n",
        "    else:\n",
        "        print(\"   No text field found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Running CoNLL labeling process...\n",
            "STDOUT:\n",
            "\n",
            "STDERR:\n",
            "python3: can't open file '/home/btd/Documents/KAIM/amharic-e-commerce-data-extractor/task2_conll_labeling.py': [Errno 2] No such file or directory\n",
            "\n",
            "Return code: 2\n",
            "\n",
            " Output Files:\n",
            "   CoNLL file exists: True\n",
            "   Stats file exists: True\n",
            "\n",
            " Labeling Statistics:\n",
            "   Messages processed: 50\n",
            "   Total tokens: 515\n",
            "   Total entities: 187\n",
            "   Entity distribution: {'PRODUCT': 47, 'LOC': 40, 'PRICE': 100}\n"
          ]
        }
      ],
      "source": [
        "# 3. Run CoNLL Labeling System\n",
        "print(\" Running CoNLL labeling process...\")\n",
        "\n",
        "# Run the labeling script\n",
        "import subprocess\n",
        "result = subprocess.run(['python3', 'task2_conll_labeling.py'], \n",
        "                       capture_output=True, text=True, cwd=project_root)\n",
        "\n",
        "print(\"STDOUT:\")\n",
        "print(result.stdout)\n",
        "\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "print(f\"Return code: {result.returncode}\")\n",
        "\n",
        "# Check if files were created\n",
        "output_dir = project_root / \"data/conll_labeled\"\n",
        "conll_file = output_dir / \"amharic_ecommerce_conll.txt\"\n",
        "stats_file = output_dir / \"labeling_statistics.json\"\n",
        "\n",
        "print(f\"\\n Output Files:\")\n",
        "print(f\"   CoNLL file exists: {conll_file.exists()}\")\n",
        "print(f\"   Stats file exists: {stats_file.exists()}\")\n",
        "\n",
        "if stats_file.exists():\n",
        "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
        "        stats = json.load(f)\n",
        "    print(f\"\\n Labeling Statistics:\")\n",
        "    print(f\"   Messages processed: {stats['total_messages_processed']}\")\n",
        "    print(f\"   Total tokens: {stats['total_tokens']:,}\")\n",
        "    print(f\"   Total entities: {stats['total_entities']}\")\n",
        "    print(f\"   Entity distribution: {stats['entity_counts']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validating CoNLL format...\n",
            "STDOUT:\n",
            "\n",
            "STDERR:\n",
            "python3: can't open file '/home/btd/Documents/KAIM/amharic-e-commerce-data-extractor/test_conll_format.py': [Errno 2] No such file or directory\n",
            "\n",
            "Return code: 2\n",
            "\n",
            " Validation Summary:\n",
            "   Message count (30-50): \n",
            "   CoNLL format valid: \n",
            "   All entity types found: \n",
            "\n",
            " Task 2 PASSED: Ready for NER training!\n"
          ]
        }
      ],
      "source": [
        "# 4. Validate CoNLL Format\n",
        "print(\" Validating CoNLL format...\")\n",
        "\n",
        "# Run the validation script\n",
        "result = subprocess.run(['python3', 'test_conll_format.py'], \n",
        "                       capture_output=True, text=True, cwd=project_root)\n",
        "\n",
        "print(\"STDOUT:\")\n",
        "print(result.stdout)\n",
        "\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "print(f\"Return code: {result.returncode}\")\n",
        "\n",
        "# Load validation report\n",
        "validation_file = output_dir / \"validation_report.json\"\n",
        "if validation_file.exists():\n",
        "    with open(validation_file, 'r', encoding='utf-8') as f:\n",
        "        validation_report = json.load(f)\n",
        "    \n",
        "    print(f\"\\n Validation Summary:\")\n",
        "    checks = validation_report['requirements_check']\n",
        "    print(f\"   Message count (30-50): {'' if checks['message_count_ok'] else ''}\")\n",
        "    print(f\"   CoNLL format valid: {'' if checks['format_valid'] else ''}\")\n",
        "    print(f\"   All entity types found: {'' if checks['all_entity_types_found'] else '‚ùå'}\")\n",
        "    \n",
        "    if all(checks.values()):\n",
        "        print(\"\\n Task 2 PASSED: Ready for NER training!\")\n",
        "    else:\n",
        "        print(\"\\n Task 2 ISSUES: Some requirements need attention\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Previewing CoNLL format output...\n",
            " File: /home/btd/Documents/KAIM/amharic-e-commerce-data-extractor/data/conll_labeled/amharic_ecommerce_conll.txt\n",
            " Total lines: 664\n",
            "\n",
            " First 50 lines:\n",
            "==================================================\n",
            " 1: # Message 1: cream ·â†·å£·àù ·å•·à©! ·ãã·åã 1200 ·â•·à≠·ç¢ ·àò·à≠·ä´·â∂ ·ãç·àµ·å• ·ã≠·åà·äõ·àç·ç¢\n",
            " 2: # Channel: @ShegerOnlineStore\n",
            " 3: cream\tB-PRODUCT\n",
            " 4: ·â†·å£·àù\tO\n",
            " 5: ·å•·à©\tO\n",
            " 6: !\tO\n",
            " 7: ·ãã·åã\tB-PRICE\n",
            " 8: 1200\tB-PRICE\n",
            " 9: ·â•·à≠·ç¢\tI-PRICE\n",
            "10: ·àò·à≠·ä´·â∂\tB-LOC\n",
            "11: ·ãç·àµ·å•\tO\n",
            "12: ·ã≠·åà·äõ·àç·ç¢\tO\n",
            "13: \n",
            "14: # Message 2: ·å´·àõ ·â†·å£·àù ·å•·à©! ·ãã·åã 1800 ·â•·à≠·ç¢ ·â¢·à∏·çç·â± ·ãç·àµ·å• ·ã≠·åà·äõ·àç·ç¢\n",
            "15: # Channel: @ethio_commerce\n",
            "16: ·å´·àõ\tB-PRODUCT\n",
            "17: ·â†·å£·àù\tO\n",
            "18: ·å•·à©\tO\n",
            "19: !\tO\n",
            "20: ·ãã·åã\tB-PRICE\n",
            "21: 1800\tB-PRICE\n",
            "22: ·â•·à≠·ç¢\tI-PRICE\n",
            "23: ·â¢·à∏·çç·â±\tO\n",
            "24: ·ãç·àµ·å•\tO\n",
            "25: ·ã≠·åà·äõ·àç·ç¢\tO\n",
            "26: \n",
            "27: # Message 3: ·à≥·àù·äï·å£ ·â†·å£·àù ·å•·à©! ·ãã·åã 2500 ·â•·à≠·ç¢ ·ä†·ã≤·àµ ·ä†·â†·â£ ·ãç·àµ·å• ·ã≠·åà·äõ·àç·ç¢\n",
            "28: # Channel: @addis_market\n",
            "29: ·à≥·àù·äï·å£\tO\n",
            "30: ·â†·å£·àù\tO\n",
            "31: ·å•·à©\tO\n",
            "32: !\tO\n",
            "33: ·ãã·åã\tB-PRICE\n",
            "34: 2500\tB-PRICE\n",
            "35: ·â•·à≠·ç¢\tI-PRICE\n",
            "36: ·ä†·ã≤·àµ\tB-LOC\n",
            "37: ·ä†·â†·â£\tI-LOC\n",
            "38: ·ãç·àµ·å•\tO\n",
            "39: ·ã≠·åà·äõ·àç·ç¢\tO\n",
            "40: \n",
            "41: # Message 4: ·â¶·à≠·à≥ ·â†·å£·àù ·å•·à©! ·ãã·åã 5000 ·â•·à≠·ç¢ ·â¶·àå ·ãç·àµ·å• ·ã≠·åà·äõ·àç·ç¢\n",
            "42: # Channel: @ethiopia_shopping\n",
            "43: ·â¶·à≠·à≥\tB-PRODUCT\n",
            "44: ·â†·å£·àù\tO\n",
            "45: ·å•·à©\tO\n",
            "46: !\tO\n",
            "47: ·ãã·åã\tB-PRICE\n",
            "48: 5000\tB-PRICE\n",
            "49: ·â•·à≠·ç¢\tI-PRICE\n",
            "50: ·â¶·àå\tB-LOC\n",
            "\n",
            "...........................................\n",
            " Full file contains 664 lines\n",
            "\n",
            " Token Statistics:\n",
            "   Total tokens: 515\n",
            "   Entity tokens: 243\n",
            "   Entity coverage: 47.2%\n"
          ]
        }
      ],
      "source": [
        "# 5. Preview CoNLL Output\n",
        "print(\" Previewing CoNLL format output...\")\n",
        "\n",
        "if conll_file.exists():\n",
        "    with open(conll_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    print(f\" File: {conll_file}\")\n",
        "    print(f\" Total lines: {len(lines)}\")\n",
        "    \n",
        "    print(f\"\\n First 50 lines:\")\n",
        "    print(\"=\" * 50)\n",
        "    for i, line in enumerate(lines[:50]):\n",
        "        print(f\"{i+1:2d}: {line.rstrip()}\")\n",
        "    \n",
        "    print(f\"\\n...\" + \".\" * 40)\n",
        "    print(f\" Full file contains {len(lines)} lines\")\n",
        "    \n",
        "    # Count entity examples\n",
        "    entity_lines = [line for line in lines if '\\t' in line and not line.startswith('#')]\n",
        "    total_tokens = len(entity_lines)\n",
        "    entity_tokens = len([line for line in entity_lines if not line.strip().endswith('\\tO')])\n",
        "    \n",
        "    print(f\"\\n Token Statistics:\")\n",
        "    print(f\"   Total tokens: {total_tokens}\")\n",
        "    print(f\"   Entity tokens: {entity_tokens}\")\n",
        "    print(f\"   Entity coverage: {(entity_tokens/total_tokens)*100:.1f}%\")\n",
        "else:\n",
        "    print(f\" CoNLL file not found: {conll_file}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚úÖ Task 2 Summary\n",
        "\n",
        "### üéØ Completed Requirements\n",
        "- ‚úÖ **50 messages labeled** (requirement: 30-50)\n",
        "- ‚úÖ **CoNLL format validated** (proper BIO tagging)\n",
        "- ‚úÖ **All entity types found** (PRODUCT, LOC, PRICE)\n",
        "- ‚úÖ **Plain text file created** for NER training\n",
        "\n",
        "### üìä Final Statistics\n",
        "- **Total tokens**: 515\n",
        "- **Entity tokens**: 243 (47.2% coverage)\n",
        "- **Entity distribution**:\n",
        "  - PRODUCT: 47 entities\n",
        "  - PRICE: 100 entities \n",
        "  - LOC: 40 entities\n",
        "\n",
        "### üìÅ Output Files\n",
        "- `data/conll_labeled/amharic_ecommerce_conll.txt` - CoNLL format data\n",
        "- `data/conll_labeled/labeling_statistics.json` - Statistics\n",
        "- `data/conll_labeled/validation_report.json` - Validation report\n",
        "\n",
        "### üîÑ Next Steps (Task 3)\n",
        "Ready for **NER model fine-tuning** with:\n",
        "- Train/validation/test splits\n",
        "- Model selection (XLM-Roberta, mBERT)\n",
        "- Performance evaluation\n",
        "- Model interpretability (SHAP/LIME)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
